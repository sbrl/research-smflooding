# Overwritten by the CLI arg
output = ""

[data]
sequence_length = 100
categories = 2

[data.paths]
# Path to the tweets.jsonl file to train on
input_train = ""
# Same as input_train, but for validation
input_validate = ""
# Path to the categories.tsv file
categories = ""
# Path to the pretrained GloVe .txt file to use
glove = ""


[model]
# The type of model. Default: lstm. Possible values: lstm, transformer
type = "lstm"

# An array of numbers, where each one represents the number of units in an LSTM layer [lstm only]
lstm_units = [ 100 ]

# An array of objects, where each represents a transformer encoding block [transformer only]
transformer_units = [
	# units_dense = the number of dense units
	# attention_heads = the number of attention heads
	# dropout = the dropout percentage between 0 and 1
	{ units_dense = 32, attention_heads = 8, dropout = 0.1 }
]
# The number of units in the last dense layer in transformer models (excluding the softmax layer at the end of course)
transformer_units_last = 20

# The dropout rate between 0 and 1 at the end of the model [transformer mode only]
dropout = 0.1

# Whether to wrap LSTM layers in a Bidirectional layer [lstm only]
bidirectional = true
# Whether to enable Batch Normalisation or not [lstm only; transformers use layer normalisation which is always enabled]
batch_normalisation = false


[train]
shuffle_buffer = 10000			# The size of the shuffle buffer in elements.
batch_size = 64					# The batch size
epochs = 50						# The number of epochs to train for
validation_data_percent = 0.2	# Percentage of data to hold back for validation

tensorboard_update_freq = 512	# Every N batches, or specify "epoch" instead

# Whether to apply SMOTE to the input training dataset. Note that the input training dataset should be UNBALANCED for this to work!
smoteify = false
