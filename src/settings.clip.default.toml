# Overwritten by the CLI arg
output = ""

[data]
categories = 2

# Only accept CLIP media labels from tweets with a confidence higher than this value (0 to 1)
clip_label_threshold=0.75

[data.paths]
# Path to the tweets.jsonl file to train on
input_train = ""
# Same as input_train, but for validation
input_validate = ""
# Same as input_test, but for the ssplit/test data
input_test = ""
# Path to the categories.tsv file
categories = ""
# Path to the directory that holds the media files
dir_media = ""

[model]
# The PyTorch device to use.
# For example, this could also be set to "cuda".
# By default, this is set to "auto", which means use CUDA if available, else fallback to the CPU.
device = "auto"
# The CLIP model name to use.
# Note that support for values other than the default value is not necessarily implemented yet.
# See https://github.com/openai/CLIP for more information.
clip = "ViT-B/32"

# The default # of units/params/etc in the tail end model we tacked on to CLIP as early fusion.
# You can change this value freely because units_in - which is set separately internally - manages the input shape of the submodel
units = 512

[train]
batch_size = 64					# The batch size
epochs = 50						# The number of epochs to train for
